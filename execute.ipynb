{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "import bs4\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import cloudscraper\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://news.detik.com/indeks/10?date=01/01/2005'\n",
    "res = requests.get(URL, verify=False)\n",
    "soup = bs4.BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all content tag : list-content__item\n",
    "contents = soup.find_all(attrs={'class':'list-content__item'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detik_metadata_mining(SECTION='finance',\n",
    "                          REQUEST_URL='https://{}.detik.com/indeks/{}?date={}',\n",
    "                          PAGE_SLEEP=0.25,\n",
    "                          DATE_SLEEP=1):\n",
    "    '''Perform detik.com data mining operations.\n",
    "    Get all articles metadata from 1 January 2005\n",
    "    to come. Saved metadata : `title`, `url`, `timestamp`.\n",
    "    \n",
    "    Capable to automatically resume progress from last\n",
    "    known timestamp (daily accuracy).\n",
    "    \n",
    "    All of the metadata saved in file named :\n",
    "    `detik_{SECTION}_results.json`, which section is\n",
    "    kwargs from this function.\n",
    "    \n",
    "    Saved file format is *.json, with structure:\n",
    "    - timestamp (daily-resolution)\n",
    "        - lists\n",
    "            - title (string)\n",
    "            - url (string)\n",
    "            - timestamp (minute-resolution) (int)\n",
    "            \n",
    "    Print daily current progress.\n",
    "    \n",
    "    Kwargs:\n",
    "        SECTION: string, optional.\n",
    "            detik.com indexed section. Valid argumens is:\n",
    "            `news`: Detik News\n",
    "            `edu`: Detik Edu\n",
    "            `finance`: Detik Finance\n",
    "            `hot`: Detik Hot\n",
    "            `inet`: Detik Inet\n",
    "            `sport`: Detik Sport\n",
    "            `oto`: Detik Oto\n",
    "            `travel`: Detik Travel\n",
    "            `sepakbola`: Detik Sepakbola\n",
    "            `food`: Detik Food\n",
    "            `health`: Detik Health\n",
    "            `wolipop`: Wolipop\n",
    "            \n",
    "        REQUEST_URL: format string with placeholder.\n",
    "            Optional. URL used to get indexed articles data\n",
    "            \n",
    "        PAGE_SLEEP: float, optional.\n",
    "            How much time intevral (in seconds) put between\n",
    "            page in particular date.\n",
    "            \n",
    "        DATE_SLEEP: float, optional.\n",
    "            How much time interval (in seconds) put between\n",
    "            date.\n",
    "            \n",
    "    Returns:\n",
    "        None.\n",
    "    '''\n",
    "    article_metadata_file = f'detik_{SECTION}_results.json'\n",
    "\n",
    "    # Read current progress from article_metadata_file\n",
    "    # or create a new blank dictionary if not exist\n",
    "    try :\n",
    "        # Try to read if the particular file exists\n",
    "        with open(article_metadata_file, 'r') as f:\n",
    "            article_metadata_dict = json.load(f)\n",
    "\n",
    "        # Check last mining progress\n",
    "        all_date_list = list(article_metadata_dict.keys())\n",
    "        all_date_list.sort(reverse=True)\n",
    "        start_date = datetime.datetime.fromtimestamp(all_date_list[0])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If file not found, create a new blank file\n",
    "        article_metadata_dict = {}\n",
    "\n",
    "        # Configure start date\n",
    "        # This is hard-coded, based on initial findings\n",
    "        # that most of the indexed content started from Jan 1, 2005\n",
    "        start_date = datetime.datetime(2005,1,1)\n",
    "\n",
    "    end_date = datetime.datetime.now()\n",
    "    day_range = end_date - start_date\n",
    "\n",
    "    for i in range(day_range.days):\n",
    "        # Track time to complete all articles in the current date\n",
    "        tick = datetime.datetime.now()\n",
    "        \n",
    "        # Assign variables\n",
    "        current_date = start_date + datetime.timedelta(days=i)\n",
    "        date_format = current_date.strftime('%d/%m/%Y')\n",
    "        page_crawl = True\n",
    "        page_number = 1\n",
    "        \n",
    "        total_articles = 0\n",
    "        while(page_crawl):\n",
    "            # Format requests URL\n",
    "            format_requests_url = REQUEST_URL.format(SECTION, page_number, date_format)\n",
    "            res = requests.get(format_requests_url, verify=False)\n",
    "            \n",
    "            # Create BeautifulSoup object for HTML\n",
    "            # structure handling\n",
    "            soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "            # Find all content tag : list-content__item\n",
    "            contents = soup.find_all(attrs={'class':'list-content__item'})\n",
    "\n",
    "            # Check contents validity in this particular page number\n",
    "            if len(contents) == 0:\n",
    "                page_crawl = False\n",
    "                break\n",
    "                \n",
    "            # Add date record to dictionary (in timestamp format)\n",
    "            article_metadata_dict[current_date.timestamp()] = []\n",
    "\n",
    "            # Loop through all available contents in particular page\n",
    "            for a in range(len(contents)):\n",
    "                # Get content title\n",
    "                content_title = contents[a].img['title']\n",
    "\n",
    "                # Get content URL\n",
    "                content_url = contents[a].a['href']\n",
    "\n",
    "                # Get content timestamp\n",
    "                ts = contents[a].find_all('span')[1]['d-time']\n",
    "\n",
    "                # Assign result(s) into dictionary\n",
    "                temp_article_result = {'title':content_title,\n",
    "                                       'url':content_url,\n",
    "                                       'timestamp':int(ts)}\n",
    "                article_metadata_dict[current_date.timestamp()].append(temp_article_result)\n",
    "                #article_metadata_dict[current_date.timestamp()]['title'] = content_title\n",
    "                #article_metadata_dict[current_date.timestamp()]['url'] = content_url\n",
    "                #article_metadata_dict[current_date.timestamp()]['timestamp'] = int(ts)\n",
    "\n",
    "                # Calculate total articles for this particular date\n",
    "                total_articles+=1\n",
    "\n",
    "            page_number+=1\n",
    "\n",
    "            # Sleep during interval between page\n",
    "            time.sleep(PAGE_SLEEP)\n",
    "\n",
    "        # Sleep during interval between date\n",
    "        time.sleep(DATE_SLEEP)\n",
    "\n",
    "        # Store current progress into file\n",
    "        with open(article_metadata_file, 'w') as f:\n",
    "            json.dump(article_metadata_dict, f)\n",
    "            \n",
    "        # Track time to complete all articles in the current date\n",
    "        tock = datetime.datetime.now()\n",
    "\n",
    "        # Print current progress\n",
    "        print(f'{i+1}/{day_range.days} : {current_date} - {total_articles} articles - time {tock-tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newssite_metadata_mining(SOURCE='detik', SECTION='finance',\n",
    "                            PAGE_SLEEP=0.25,\n",
    "                            DATE_SLEEP=1,\n",
    "                            START_DATE=(2005,1,1)):\n",
    "    '''Perform detik.com data mining operations.\n",
    "    Get all articles metadata from 1 January 2005\n",
    "    to come. Saved metadata : `title`, `url`, `timestamp`.\n",
    "    \n",
    "    Capable to automatically resume progress from last\n",
    "    known timestamp (daily accuracy).\n",
    "    \n",
    "    All of the metadata saved in file named :\n",
    "    `detik_{SECTION}_results.json`, which section is\n",
    "    kwargs from this function.\n",
    "    \n",
    "    Saved file format is *.json, with structure:\n",
    "    - timestamp (daily-resolution)\n",
    "        - lists\n",
    "            - title (string)\n",
    "            - url (string)\n",
    "            - timestamp (minute-resolution) (int)\n",
    "            \n",
    "    Print daily current progress.\n",
    "    \n",
    "    Kwargs:\n",
    "        SECTION: string, optional.\n",
    "            detik.com indexed section. Valid argumens is:\n",
    "            `news`: Detik News\n",
    "            `edu`: Detik Edu\n",
    "            `finance`: Detik Finance\n",
    "            `hot`: Detik Hot\n",
    "            `inet`: Detik Inet\n",
    "            `sport`: Detik Sport\n",
    "            `oto`: Detik Oto\n",
    "            `travel`: Detik Travel\n",
    "            `sepakbola`: Detik Sepakbola\n",
    "            `food`: Detik Food\n",
    "            `health`: Detik Health\n",
    "            `wolipop`: Wolipop\n",
    "            \n",
    "        REQUEST_URL: format string with placeholder.\n",
    "            Optional. URL used to get indexed articles data\n",
    "            \n",
    "        PAGE_SLEEP: float, optional.\n",
    "            How much time intevral (in seconds) put between\n",
    "            page in particular date.\n",
    "            \n",
    "        DATE_SLEEP: float, optional.\n",
    "            How much time interval (in seconds) put between\n",
    "            date.\n",
    "            \n",
    "    Returns:\n",
    "        None.\n",
    "    '''\n",
    "    article_metadata_file = f'{SOURCE}_{SECTION}_results.json'\n",
    "\n",
    "    # Read current progress from article_metadata_file\n",
    "    # or create a new blank dictionary if not exist\n",
    "    try :\n",
    "        # Try to read if the particular file exists\n",
    "        with open(article_metadata_file, 'r') as f:\n",
    "            article_metadata_dict = json.load(f)\n",
    "\n",
    "        # Check last mining progress\n",
    "        all_date_list = list(article_metadata_dict.keys())\n",
    "        all_date_list.sort(reverse=True)\n",
    "        start_date = datetime.datetime.fromtimestamp(float(all_date_list[0]))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If file not found, create a new blank file\n",
    "        article_metadata_dict = {}\n",
    "\n",
    "        # Configure start date\n",
    "        # This is hard-coded, based on initial findings\n",
    "        # that most of the indexed content started from Jan 1, 2005\n",
    "        start_date = datetime.datetime(*START_DATE)\n",
    "\n",
    "    end_date = datetime.datetime.now()\n",
    "    day_range = end_date - start_date\n",
    "\n",
    "    for i in range(day_range.days):\n",
    "        # Track time to complete all articles in the current date\n",
    "        tick = datetime.datetime.now()\n",
    "        \n",
    "        # Assign variables\n",
    "        current_date = start_date + datetime.timedelta(days=i)\n",
    "        page_crawl = True\n",
    "        page_number = 1\n",
    "        \n",
    "        total_articles = 0\n",
    "        while(page_crawl):\n",
    "            try:\n",
    "                # Perform data mining\n",
    "                contents = process_switcher(SOURCE, SECTION, page_number, current_date)\n",
    "\n",
    "                # Check contents validity in this particular page number\n",
    "                if len(contents) == 0:\n",
    "                    page_crawl = False\n",
    "                    break\n",
    "\n",
    "                # Add date record to dictionary (in timestamp format)\n",
    "                article_metadata_dict[current_date.timestamp()] = []\n",
    "\n",
    "                # Loop through all available contents in particular page\n",
    "                for content in contents:\n",
    "                    # Assign result(s) into dictionary\n",
    "                    temp_article_result = {}\n",
    "                    for key in content.keys():\n",
    "                        temp_article_result[key] = content[key]\n",
    "                    article_metadata_dict[current_date.timestamp()].append(temp_article_result)\n",
    "\n",
    "                    # Calculate total articles for this particular date\n",
    "                    total_articles+=1\n",
    "                \n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            page_number+=1\n",
    "\n",
    "            # Sleep during interval between page\n",
    "            time.sleep(PAGE_SLEEP)\n",
    "\n",
    "        # Sleep during interval between date\n",
    "        time.sleep(DATE_SLEEP)\n",
    "\n",
    "        # Store current progress into file\n",
    "        with open(article_metadata_file, 'w') as f:\n",
    "            json.dump(article_metadata_dict, f)\n",
    "            \n",
    "        # Track time to complete all articles in the current date\n",
    "        tock = datetime.datetime.now()\n",
    "\n",
    "        # Print current progress\n",
    "        print(f'{SOURCE} {SECTION} {i+1}/{day_range.days} : {current_date} - {total_articles} articles - time {tock-tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_formatter(month_text):\n",
    "    '''Convert short month spell from `id` to `en`.\n",
    "    '''\n",
    "    if month_text == 'Mei':\n",
    "        return 'May'\n",
    "    elif month_text == 'Agu':\n",
    "        return 'Aug'\n",
    "    elif month_text == 'Okt':\n",
    "        return 'Oct'\n",
    "    elif month_text == 'Des':\n",
    "        return 'Dec'\n",
    "    else:\n",
    "        return month_text\n",
    "\n",
    "def process_switcher(SOURCE, SECTION, page_number, current_date):\n",
    "    '''\n",
    "    Switch between various mining processor and\n",
    "    return crawled contents in dictionary format\n",
    "    '''\n",
    "    identifier = f'{SOURCE}_{SECTION}'\n",
    "    if ((identifier == 'detik_news') or\n",
    "        (identifier == 'detik_finance') or\n",
    "        (identifier == 'detik_hot') or\n",
    "        (identifier == 'detik_sport') or\n",
    "        (identifier == 'detik_oto')):\n",
    "        contents = detik_general_processor(SECTION, page_number, current_date)\n",
    "\n",
    "    elif identifier == 'detik_edu':\n",
    "        contents = detik_edu_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif identifier == 'detik_inet':\n",
    "        contents = detik_inet_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif identifier == 'detik_travel':\n",
    "        contents = detik_travel_processor(SECTION, page_number, current_date)\n",
    "    \n",
    "    elif ((identifier == 'detik_food') or\n",
    "          (identifier == 'detik_health')):\n",
    "        contents = detik_food_health_processor(SECTION, page_number, current_date)\n",
    "    \n",
    "    elif identifier == 'detik_wolipop':\n",
    "        contents = detik_wolipop_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif SOURCE == 'kompas':\n",
    "        contents = kompasdotcom_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif SOURCE == 'bisnis':\n",
    "        contents = bisnisdotcom_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif SOURCE == 'kontan':\n",
    "        contents = kontan_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    elif SOURCE == 'cnbc':\n",
    "        # Try desktop version of the web\n",
    "        try:\n",
    "            contents = cnbcindonesia_processor(SECTION, page_number, current_date)\n",
    "        # If the response is mobile version,\n",
    "        # use this version of processor\n",
    "        except IndexError:\n",
    "            contents = cnbcindonesia_alt_processor(SECTION, page_number, current_date)\n",
    "        \n",
    "    return contents\n",
    "\n",
    "def detik_general_processor(section, \n",
    "                            page, \n",
    "                            current_date, \n",
    "                            DATE_FORMAT='%m/%d/%Y',\n",
    "                            REQUEST_URL='https://{0}.detik.com/indeks/{1}?date={2}'):\n",
    "    '''\n",
    "    Valid for finance, hot, news, sport, oto\n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    formatted_request_url = REQUEST_URL.format(section, page, date_compatible_format)\n",
    "    res = requests.get(formatted_request_url, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'list-content__item'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        temp_result = {'title':content.img['title'],\n",
    "                       'url':content.a['href'],\n",
    "                       'timestamp':int(content.find_all('span')[1]['d-time'])}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def detik_edu_processor(section,\n",
    "                        page,\n",
    "                        current_date,\n",
    "                        DATE_FORMAT='%m/%d/%Y',\n",
    "                        REQUEST_URL = 'https://www.detik.com/{0}/indeks/{1}?date={2}'):\n",
    "    return detik_general_processor(section, page, current_date, DATE_FORMAT, REQUEST_URL)\n",
    "\n",
    "def detik_inet_processor(section,\n",
    "                         page,\n",
    "                         current_date,\n",
    "                         DATE_FORMAT='%d-%m-%Y',\n",
    "                         REQUEST_URL='https://{0}.detik.com/indeks/{1}?date={2}'):\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    formatted_request_url = REQUEST_URL.format(section, page, date_compatible_format)\n",
    "    res = requests.get(formatted_request_url, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'list-content__item'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        temp_result = {'title':content.a['dtr-ttl'],\n",
    "                       'url':content['i-link'],\n",
    "                       'timestamp':int(bs4.BeautifulSoup(content['i-info']).span['d-time'])}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def detik_travel_processor(section,\n",
    "                           page,\n",
    "                           current_date,\n",
    "                           DATE_FORMAT='%m/%d/%Y',\n",
    "                           REQUEST_URL='https://{0}.detik.com/indeks/{1}?date={2}'):\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    formatted_request_url = REQUEST_URL.format(section, page, date_compatible_format)\n",
    "    res = requests.get(formatted_request_url, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'list__news--trigger'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        date_list_slice = (content.div.div.text).split(' ')[1:5]\n",
    "        date_list_slice[1] = month_formatter(date_list_slice[1])\n",
    "        datetime_formatted = datetime.datetime.strptime(' '.join(date_list_slice), '%d %b %Y %H:%M')\n",
    "\n",
    "        temp_result = {'title':content.a['dtr-ttl'],\n",
    "                       'url':content.a['href'],\n",
    "                       'timestamp':int(datetime_formatted.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def detik_food_health_processor(section,\n",
    "                                page,\n",
    "                                current_date,\n",
    "                                DATE_FORMAT='%m/%d/%Y',\n",
    "                                REQUEST_URL='https://{0}.detik.com/indeks/{1}?date={2}'):\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    formatted_request_url = REQUEST_URL.format(section, page, date_compatible_format)\n",
    "    res = requests.get(formatted_request_url, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all('article')\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        date_list_slice = (content.span.text).split(' ')[1:5]\n",
    "        date_list_slice[1] = month_formatter(date_list_slice[1])\n",
    "        datetime_formatted = datetime.datetime.strptime(' '.join(date_list_slice), '%d %b %Y %H:%M')\n",
    "\n",
    "        temp_result = {'title':content.h2.text,\n",
    "                       'url':content.a['href'],\n",
    "                       'timestamp':int(datetime_formatted.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def detik_wolipop_processor(section,\n",
    "                            page,\n",
    "                            current_date,\n",
    "                            DATE_FORMAT='%m/%d/%Y',\n",
    "                            REQUEST_URL='https://{0}.detik.com/indeks/{1}?date={2}'):\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    formatted_request_url = REQUEST_URL.format(section, page, date_compatible_format)\n",
    "    res = requests.get(formatted_request_url, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'text'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        date_list_slice = (content.span.text).split(' ')[1:5]\n",
    "        date_list_slice[1] = month_formatter(date_list_slice[1])\n",
    "        datetime_formatted = datetime.datetime.strptime(' '.join(date_list_slice), '%d %b %Y %H:%M')\n",
    "\n",
    "        temp_result = {'title':content.h3.a.text.replace('\\n', '').strip(),\n",
    "                       'url':content.a['href'],\n",
    "                       'timestamp':int(datetime_formatted.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results   \n",
    "\n",
    "def kompasdotcom_processor(section, \n",
    "                           page, \n",
    "                           current_date, \n",
    "                           DATE_FORMAT='%Y-%m-%d'):\n",
    "    '''\n",
    "    Valid for `all`, `news`, `megapolitan`,\n",
    "    `nasional`, `regional`, `global`, `tren`,\n",
    "    `health`, `food`, `edukasi`, `money`,\n",
    "    `tekno`, `lifestyle`, `homey`, `properti`,\n",
    "    `bola`, `travel`, `otomotif`, `sains`,\n",
    "    `hype`, `jeo`, `health`, `skola`, `stori`,\n",
    "    `konsultasihukum`, `headline`, `terpopuler`,\n",
    "    `sorotan`, `topik-pilihan`\n",
    "    \n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    REQUEST_URL = f'https://indeks.kompas.com/?site={section}&date={date_compatible_format}&page={page}'\n",
    "\n",
    "    res = requests.get(REQUEST_URL, verify=False)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'article__list'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        raw_date = content.find(attrs={'class':'article__date'}).text\n",
    "        raw_date = raw_date.replace(',','').replace(' WIB', '')\n",
    "        on_datetime = datetime.datetime.strptime(raw_date, '%d/%m/%Y %H:%M')\n",
    "        temp_result = {'section':content.find(attrs={'class':'article__subtitle--inline'}).text,\n",
    "                       'title':content.div.div.a.img['alt'],\n",
    "                       'url':content.div.div.a['href'],\n",
    "                       'timestamp':int(on_datetime.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def bisnisdotcom_processor(section, \n",
    "                           page, \n",
    "                           current_date, \n",
    "                           DATE_FORMAT='%Y-%m-%d'):\n",
    "    '''\n",
    "    Section value : (section-label) -> (section-value)\n",
    "    `Semua Kanal` : `0`\n",
    "    `Market` : `194`\n",
    "    `Finansial` : `5`\n",
    "    `Ekonomi` : `43`\n",
    "    `Kabar24` : `186`\n",
    "    `Teknologi` : `277`\n",
    "    `Lifestyle` : `197`\n",
    "    `Entrepreneur` : `258`\n",
    "    `Travel` : `222`\n",
    "    `Sport` : `57`\n",
    "    `Bola` : `392`\n",
    "    `Otomotif` : `272`\n",
    "    `Jakarta` : `382`\n",
    "    `Bandung` : `548`\n",
    "    `Banten` : `420`\n",
    "    `Semarang` : `528`\n",
    "    `Surabaya` : `526`\n",
    "    `Bali` : `529`\n",
    "    `Sumatra` : `527`\n",
    "    `Kalimantan` : `406`\n",
    "    `Sulawesi` : `530`\n",
    "    `Papua` : `413`\n",
    "    `Koran` : `242`\n",
    "    `Infografik` : `547`\n",
    "    `Ramadan` : `390`\n",
    "    `Bisnis TV` : `551`\n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    REQUEST_URL = f'https://www.bisnis.com/index?c={section}&d={date_compatible_format}&per_page={page}'\n",
    "\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    res = scraper.get(REQUEST_URL)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Contents length check\n",
    "    content_length = soup.find_all(attrs={'class':'list-news', 'class':'indeks-new'})\n",
    "    if len(content_length) > 0:\n",
    "        # In empty page, the article placeholder may\n",
    "        # filled with \"Tidak ada berita\", that send\n",
    "        # false alarm into the system\n",
    "        try:\n",
    "            contents = soup.find_all(attrs={'class':'list-news', 'class':'indeks-new'})[0].find_all('li')\n",
    "            results = []\n",
    "            for content in contents:\n",
    "                raw_date = (content.find(attrs={'class':'date'}).text).strip().replace(' WIB', '')\n",
    "                on_datetime = datetime.datetime.strptime(raw_date, '%d %b %Y | %H:%M')\n",
    "                temp_result = {'section':content.find(attrs={'class':'wrapper-description'}).div.a['href'],\n",
    "                               'title':content.div.a['title'],\n",
    "                               'url':content.div.a['href'],\n",
    "                               'timestamp':int(on_datetime.timestamp())}\n",
    "                results.append(temp_result)\n",
    "            return results\n",
    "        except AttributeError:\n",
    "            return content_length\n",
    "    else:\n",
    "        return content_length\n",
    "    \n",
    "def kontan_processor(section, \n",
    "                     page, \n",
    "                     current_date, \n",
    "                     DATE_FORMAT='&tanggal=%d&bulan=%m&tahun=%Y',\n",
    "                     ARTICLES_PER_PAGE=20):\n",
    "    '''\n",
    "    Section value : (section-label) -> (section-value)\n",
    "    `Semua Artikel` : ``\n",
    "    `Nasional` : `nasional`\n",
    "    `Keuangan` : `keuangan`\n",
    "    `Investasi` : `investasi`\n",
    "    `Industri` : `industri`\n",
    "    `Internasional` : `internasional`\n",
    "    `Peluang Usaha` : `peluangusaha`\n",
    "    `Personal Finance` : `personalfinance`\n",
    "    `English` : `english`\n",
    "    `Lifestyle` : `lifestyle`\n",
    "    `Fokus` : `fokus`\n",
    "    `Piala Eropa` : `pialaeropa`\n",
    "    `Regional` : `regional`\n",
    "    `Yangter` : `yangter`\n",
    "    `Kesehatan` : `kesehatan`\n",
    "    `Cari Tahu` : `caritahu`\n",
    "    `Analisis` : `analisis`\n",
    "    `Executive` : `executive`\n",
    "    `Kolom` : `kolom`\n",
    "    `Kilas Kementerian` : `kilaskementerian`\n",
    "    `Infografik` : `infografik`\n",
    "    `Insight` : `insight`\n",
    "    `Cek Fakta` : `cekfakta`\n",
    "    `Ads` : `ads`\n",
    "    `seremonia` : `seremonia`\n",
    "    `Native` : `native`\n",
    "    `Adv` : `adv`\n",
    "    `Export Expert` : `exportexpert`\n",
    "    `Tabloid` : `tabloid`\n",
    "    `Kilas Korporasi` : `kilaskorporasi`\n",
    "    `Edsus` : `edsus`\n",
    "    `Kontan TV` : `tv`\n",
    "    `Stock Setup` : `stocksetup`\n",
    "    `BelanjaOn` : `belanjaon`\n",
    "    `News Setup` : `newssetup`\n",
    "    `Film On` : `filmon`\n",
    "    `Kiat On` : `kiaton`\n",
    "    `Sport Setup` : `sportsetup`\n",
    "    `momsmoney.id` : `momsmoneyid`\n",
    "    \n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    REQUEST_URL = f'https://www.kontan.co.id/search/indeks?kanal={section}{date_compatible_format}&pos=indeks&per_page={(page - 1) * ARTICLES_PER_PAGE}'\n",
    "\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    res = scraper.get(REQUEST_URL)\n",
    "    \n",
    "    # Check if the status OK or NOT OK\n",
    "    if not res.ok:\n",
    "        print('Server error 500. Try 5 times with 10 sec interval to make sure its not a fluke.')\n",
    "        res_ok = False\n",
    "        for i in range(5):\n",
    "            res = scraper.get(REQUEST_URL)\n",
    "            if res.ok:\n",
    "                res_ok = True\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        # If still `Server Error 500`, \n",
    "        # return blank lists.\n",
    "        if not res_ok:\n",
    "            return []\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'data-offset':'20'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        # Isolate datetime constructor from web\n",
    "        date_list_slice = []\n",
    "        date_list_slice_temp = (content.find(attrs={'class':'fs14','class':'ff-opensans'}).find(attrs={'class':'font-gray'}).text).split(' ')\n",
    "        \n",
    "        # Construct timedelta params\n",
    "        search_keys = ('Tahun', 'Bulan', 'Hari', 'Jam', 'Menit')\n",
    "        timedelta_constructor = []\n",
    "        for search_key in search_keys:\n",
    "            try:\n",
    "                index = date_list_slice_temp.index(search_key)\n",
    "                timedelta_constructor.append(int(date_list_slice_temp[index-1]))\n",
    "            except ValueError:\n",
    "                timedelta_constructor.append(0)\n",
    "        \n",
    "        # Calculate article publish date relative to current datetime\n",
    "        date_now = datetime.datetime.now()\n",
    "        news_date = date_now - relativedelta(years=timedelta_constructor[0], months=timedelta_constructor[1], days=timedelta_constructor[2], hours=timedelta_constructor[3], minutes=timedelta_constructor[4])\n",
    "        \n",
    "        temp_result = {'section':content.find(attrs={'class':'linkto-orange', 'class':'hrf-gede', 'class':'mar-r-5'}).a.text,\n",
    "                       'title':content.find('h1').a.text,\n",
    "                       'url':'https:' + content.a['href'],\n",
    "                       'timestamp':int(news_date.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def cnbcindonesia_processor(section, \n",
    "                            page, \n",
    "                            current_date, \n",
    "                            DATE_FORMAT='%Y/%m/%d'):\n",
    "    '''\n",
    "    Valid for all articles index only.    \n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    REQUEST_URL = f'https://www.cnbcindonesia.com/indeks/{page}?date={date_compatible_format}'\n",
    "\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    res = scraper.get(REQUEST_URL)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'gtm_indeks_feed'})\n",
    "    contents = contents[0].find_all('li')\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        # Fetch URL, remove \"https://\"\n",
    "        time_from_url = (content.article.a['href'])\n",
    "        time_from_url = time_from_url.replace('https://', '')\n",
    "\n",
    "        # Split by folder \"/\"\n",
    "        time_from_url = time_from_url.split('/')\n",
    "\n",
    "        # Get 3rd content in the folder (time indices)\n",
    "        time_from_url = time_from_url[2]\n",
    "\n",
    "        # Split by \"-\", to get year-month-day-hour-minute-seconds\n",
    "        # published time of that article\n",
    "        time_from_url = time_from_url.split('-')\n",
    "        time_from_url = time_from_url[0]\n",
    "        time_from_url = datetime.datetime.strptime(time_from_url, '%Y%m%d%H%M%S')       \n",
    "        \n",
    "        temp_result = {'section':content.find(attrs={'class':'label'}).text,\n",
    "                       'title':content.find('h2').text,\n",
    "                       'url':content.article.a['href'],\n",
    "                       'timestamp':int(time_from_url.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results\n",
    "\n",
    "def cnbcindonesia_alt_processor(section, \n",
    "                                page, \n",
    "                                current_date, \n",
    "                                DATE_FORMAT='%Y/%m/%d'):\n",
    "    '''\n",
    "    *ALTERNATIVE VERSION*\n",
    "    Fetch mobile version of the web\n",
    "    that have different structure compared\n",
    "    to the desktop version.\n",
    "    \n",
    "    Valid for all articles index only.    \n",
    "    '''\n",
    "    date_compatible_format = current_date.strftime(DATE_FORMAT)\n",
    "    REQUEST_URL = f'https://www.cnbcindonesia.com/indeks/{page}?date={date_compatible_format}'\n",
    "\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    res = scraper.get(REQUEST_URL)\n",
    "\n",
    "    # Create BeautifulSoup object for HTML\n",
    "    # structure handling\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "    # Find all content tag : list-content__item\n",
    "    contents = soup.find_all(attrs={'class':'list__item'})\n",
    "\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        # Fetch URL, remove \"https://\"\n",
    "        time_from_url = content.a['href']\n",
    "        time_from_url = time_from_url.replace('https://', '')\n",
    "\n",
    "        # Split by folder \"/\"\n",
    "        time_from_url = time_from_url.split('/')\n",
    "\n",
    "        # Get 3rd content in the folder (time indices)\n",
    "        time_from_url = time_from_url[2]\n",
    "\n",
    "        # Split by \"-\", to get year-month-day-hour-minute-seconds\n",
    "        # published time of that article\n",
    "        time_from_url = time_from_url.split('-')\n",
    "        time_from_url = time_from_url[0]\n",
    "        time_from_url = datetime.datetime.strptime(time_from_url, '%Y%m%d%H%M%S')       \n",
    "        \n",
    "        temp_result = {'section':content.find(attrs={'class':'sub'}).text,\n",
    "                       'title':content.find('h4').a.text,\n",
    "                       'url':content.a['href'],\n",
    "                       'timestamp':int(time_from_url.timestamp())}\n",
    "        results.append(temp_result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-c2f97992041c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                      \u001b[0mDATE_FORMAT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'&tanggal=%d&bulan=%m&tahun=%Y'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                      ARTICLES_PER_PAGE=20)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mre_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "section = ''\n",
    "page = 1\n",
    "current_date = datetime.datetime(2011,1,2)\n",
    "re_data = kontan_processor(section, \n",
    "                     page, \n",
    "                     current_date, \n",
    "                     DATE_FORMAT='&tanggal=%d&bulan=%m&tahun=%Y',\n",
    "                     ARTICLES_PER_PAGE=20)\n",
    "re_data[3].a.div.img['alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secara hitungan tahun, Wallstreet masuk zona hijau\n",
      "17 BUMN sumbangkan kapitalisasi di pasar modal sebesar 26%\n",
      "Pemerintah jamin pasokan beras bakal aman\n",
      "Kementan perluas lahan lapas 18.000 hektar\n",
      "Kementan targetkan produksi kapas 33.000 ton per tahun\n",
      "Revitalisasi macet dan curah hujan tinggi bikin produksi gula jadi mini\n",
      "Produksi gula 2011 diperkirakan hanya 2,7 juta ton\n",
      "Bapepam -LK terbitkan pedoman pengelolaan dan kontrak berbentuk KIK\n"
     ]
    }
   ],
   "source": [
    "contents = re_data\n",
    "results = []\n",
    "for content in contents:\n",
    "    print(content.find('h1').a.text)\n",
    "    # Isolate datetime constructor from web\n",
    "    date_list_slice = []\n",
    "    date_list_slice_temp = (content.find(attrs={'class':'fs14','class':'ff-opensans'}).find(attrs={'class':'font-gray'}).text).split(' ')\n",
    "\n",
    "    # Construct timedelta params\n",
    "    search_keys = ('Tahun', 'Bulan', 'Hari', 'Jam', 'Menit')\n",
    "    timedelta_constructor = []\n",
    "    for search_key in search_keys:\n",
    "        try:\n",
    "            index = date_list_slice_temp.index(search_key)\n",
    "            timedelta_constructor.append(int(date_list_slice_temp[index-1]))\n",
    "        except ValueError:\n",
    "            timedelta_constructor.append(0)\n",
    "\n",
    "    # Calculate article publish date relative to current datetime\n",
    "    date_now = datetime.datetime.now()\n",
    "    news_date = date_now - relativedelta(years=timedelta_constructor[0], months=timedelta_constructor[1], days=timedelta_constructor[2], hours=timedelta_constructor[3], minutes=timedelta_constructor[4])\n",
    "\n",
    "    temp_result = {'section':content.find(attrs={'class':'linkto-orange', 'class':'hrf-gede', 'class':'mar-r-5'}).a.text,\n",
    "                   'title':content.find('h1').a.text,\n",
    "                   'url':'https:' + content.a['href'],\n",
    "                   'timestamp':int(news_date.timestamp())}\n",
    "    results.append(temp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-59d652542308>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "content.a.div.img['alt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yang mau dicari di tahap 1 :\n",
    "- Judul berita\n",
    "- URL berita\n",
    "- Tanggal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompas test\n",
    "URL = 'https://www.cnbcindonesia.com/indeks/2?date=2018/01/08'\n",
    "scraper = cloudscraper.create_scraper()\n",
    "res = scraper.get(URL)\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "contents_parent = soup.find_all(attrs={'class':'gtm_indeks_feed'})\n",
    "#contents_parent[0].find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<article class=\"list__item clearfix\">\n",
      " <a class=\"list__item__photo box_img pull-left\" href=\"https://www.cnbcindonesia.com/news/20180108170038-4-911/masih-urus-izin-kantor-apple-di-bsd-belum-terealisasi\">\n",
      "  <picture class=\"img_con lqd\">\n",
      "   <img alt=\"Masih Urus Izin, Kantor Apple di BSD Belum Terealisasi\" src=\"https://akcdn.detik.net.id/visual/2018/01/08/16f7486a-88dd-4eda-8739-aba1c752474a_43.jpeg?w=95&amp;q=90\"/>\n",
      "  </picture>\n",
      " </a>\n",
      " <div class=\"list__item__desc pull-left\">\n",
      "  <h4>\n",
      "   <a href=\"https://www.cnbcindonesia.com/news/20180108170038-4-911/masih-urus-izin-kantor-apple-di-bsd-belum-terealisasi\">\n",
      "    Masih Urus Izin, Kantor Apple di BSD Belum Terealisasi\n",
      "   </a>\n",
      "  </h4>\n",
      "  <span class=\"sub\">\n",
      "   News\n",
      "  </span>\n",
      "  <span class=\"date\">\n",
      "   Senin, 08/01/2018 17:00 WIB\n",
      "  </span>\n",
      " </div>\n",
      "</article>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = soup.find_all(attrs={'class':'list__item'})\n",
    "print(contents[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'News'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0].find(attrs={'class':'sub'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = contents[0].a['href']\n",
    "title = ccontents[0].find('h4').a.text\n",
    "\n",
    "# Fetch URL, remove \"https://\"\n",
    "time_from_url = (contents[0].a['href'])\n",
    "time_from_url = time_from_url.replace('https://', '')\n",
    "\n",
    "# Split by folder \"/\"\n",
    "time_from_url = time_from_url.split('/')\n",
    "\n",
    "# Get 3rd content in the folder (time indices)\n",
    "time_from_url = time_from_url[2]\n",
    "\n",
    "# Split by \"-\", to get year-month-day-hour-minute-seconds\n",
    "# published time of that article\n",
    "time_from_url = time_from_url.split('-')\n",
    "time_from_url = time_from_url[0]\n",
    "time_from_url = datetime.datetime.strptime(time_from_url, '%Y%m%d%H%M%S')\n",
    "time_from_url\n",
    "\n",
    "section = contents[0].find(attrs={'class':'sub'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|', '9', 'Tahun', '17', 'Hari', '9', 'Jam', '38', 'Menit', 'lalu']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list_slice = []\n",
    "date_list_slice_temp = (contents[0].find(attrs={'class':'fs14','class':'ff-opensans'}).find(attrs={'class':'font-gray'}).text).split(' ')\n",
    "date_list_slice_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 0, 17, 9, 38]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_keys = ('Tahun', 'Bulan', 'Hari', 'Jam', 'Menit')\n",
    "timedelta_constructor = []\n",
    "for search_key in search_keys:\n",
    "    try:\n",
    "        index = date_list_slice_temp.index(search_key)\n",
    "        timedelta_constructor.append(int(date_list_slice_temp[index-1]))\n",
    "    except ValueError:\n",
    "        timedelta_constructor.append(0)\n",
    "timedelta_constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2012, 8, 14, 23, 55, 19, 276531)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "news_date = date_now - relativedelta(years=timedelta_constructor[0], months=timedelta_constructor[1], days=timedelta_constructor[2], hours=timedelta_constructor[3], minutes=timedelta_constructor[4])\n",
    "news_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REGIONAL'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0].find(attrs={'class':'article__subtitle--inline'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Pilih Kanal` : ``\n",
      "`Nasional` : `nasional`\n",
      "`Keuangan` : `keuangan`\n",
      "`Investasi` : `investasi`\n",
      "`Industri` : `industri`\n",
      "`Internasional` : `internasional`\n",
      "`Peluang Usaha` : `peluangusaha`\n",
      "`Personal Finance` : `personalfinance`\n",
      "`English` : `english`\n",
      "`Lifestyle` : `lifestyle`\n",
      "`Fokus` : `fokus`\n",
      "`Piala Eropa` : `pialaeropa`\n",
      "`Regional` : `regional`\n",
      "`Yangter` : `yangter`\n",
      "`Kesehatan` : `kesehatan`\n",
      "`Cari Tahu` : `caritahu`\n",
      "`Analisis` : `analisis`\n",
      "`Executive` : `executive`\n",
      "`Kolom` : `kolom`\n",
      "`Kilas Kementerian` : `kilaskementerian`\n",
      "`Infografik` : `infografik`\n",
      "`Insight` : `insight`\n",
      "`Cek Fakta` : `cekfakta`\n",
      "`Ads` : `ads`\n",
      "`seremonia` : `seremonia`\n",
      "`Native` : `native`\n",
      "`Adv` : `adv`\n",
      "`Export Expert` : `exportexpert`\n",
      "`Tabloid` : `tabloid`\n",
      "`Kilas Korporasi` : `kilaskorporasi`\n",
      "`Edsus` : `edsus`\n",
      "`Kontan TV` : `tv`\n",
      "`Stock Setup` : `stocksetup`\n",
      "`BelanjaOn` : `belanjaon`\n",
      "`News Setup` : `newssetup`\n",
      "`Film On` : `filmon`\n",
      "`Kiat On` : `kiaton`\n",
      "`Sport Setup` : `sportsetup`\n",
      "`momsmoney.id` : `momsmoneyid`\n"
     ]
    }
   ],
   "source": [
    "soup_c = '''\n",
    "            <select placeholder=\"Tanggal\" style=\"padding: 6px 11px;\" name=\"kanal\"> <option value=\"\"> Pilih Kanal </option> <option value=\"nasional\">Nasional</option> <option value=\"keuangan\">Keuangan</option> <option value=\"investasi\">Investasi</option> <option value=\"industri\">Industri</option> <option value=\"internasional\">Internasional</option> <option value=\"peluangusaha\">Peluang Usaha</option> <option value=\"personalfinance\">Personal Finance</option> <option value=\"english\">English</option> <option value=\"lifestyle\">Lifestyle</option> <option value=\"fokus\">Fokus</option> <option value=\"pialaeropa\">Piala Eropa</option> <option value=\"regional\">Regional</option> <option value=\"yangter\">Yangter</option> <option value=\"kesehatan\">Kesehatan</option> <option value=\"caritahu\">Cari Tahu</option> <option value=\"analisis\">Analisis</option> <option value=\"executive\">Executive</option> <option value=\"kolom\">Kolom</option> <option value=\"kilaskementerian\">Kilas Kementerian</option> <option value=\"infografik\">Infografik</option> <option value=\"insight\">Insight</option> <option value=\"cekfakta\">Cek Fakta</option> <option value=\"ads\">Ads</option> <option value=\"seremonia\">seremonia</option> <option value=\"native\">Native</option> <option value=\"adv\">Adv</option> <option value=\"exportexpert\">Export Expert </option> <option value=\"tabloid\">Tabloid</option> <option value=\"kilaskorporasi\">Kilas Korporasi</option> <option value=\"edsus\">Edsus</option> <option value=\"tv\">Kontan TV</option> <option value=\"stocksetup\">Stock Setup</option> <option value=\"belanjaon\">BelanjaOn</option> <option value=\"newssetup\">News Setup</option> <option value=\"filmon\">Film On</option> <option value=\"kiaton\">Kiat On</option> <option value=\"sportsetup\">Sport Setup</option> <option value=\"momsmoneyid\">momsmoney.id</option> </select>\n",
    "'''\n",
    "\n",
    "soup_c = bs4.BeautifulSoup(soup_c)\n",
    "c_contents = soup_c.find_all('option')\n",
    "for c_content in c_contents:\n",
    "    value = c_content['value']\n",
    "    text = c_content.text\n",
    "    print_format = f\"\"\"`{text.strip()}` : `{value}`\"\"\"\n",
    "    print(print_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2013, 5, 1, 15, 23)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.fromtimestamp(1367396580)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = '0'\n",
    "page = '2'\n",
    "current_date = datetime.datetime(2010,12,2)\n",
    "soup = bisnisdotcom_processor(section, page, current_date, DATE_FORMAT='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = soup.find_all(attrs={'class':'list-news', 'class':'indeks-new'})[0].find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li><h2 style=\"font-size:14px; font-style:italic; padding:50px;\"> Tidak ada berita</h2></li>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all(attrs={'class':'list-news', 'class':'indeks-new'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '//investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik': No schema supplied. Perhaps you meant http:////investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-6981e67a75f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         )\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m         )\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3-2018\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '//investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik': No schema supplied. Perhaps you meant http:////investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik?"
     ]
    }
   ],
   "source": [
    "a = requests.get('//investasi.kontan.co.id/news/dow-dan-sp-naik-karena-data-ritel-as-membaik')\n",
    "a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
